{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecDc0quRafr7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "yyOMSliMa0Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import sys   \n",
        "from sklearn.model_selection import train_test_split     \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Edwien0la0Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/train.csv\")"
      ],
      "metadata": {
        "id": "TpWA6ADva0Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df,val_df=train_test_split(df,test_size=0.05)\n",
        "train_df.shape,val_df.shape"
      ],
      "metadata": {
        "id": "eEFEoP-7a0Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "EPLoYVnZa0QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_COLUMNS=['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']"
      ],
      "metadata": {
        "id": "vSor5Iyea0Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[LABEL_COLUMNS].sum()"
      ],
      "metadata": {
        "id": "lXnmMa1Sa0VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(train_df[LABEL_COLUMNS])"
      ],
      "metadata": {
        "id": "a1j2Jmcga0YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_toxic=train_df[train_df[LABEL_COLUMNS].sum(axis=1)>0]\n",
        "train_clean=train_df[train_df[LABEL_COLUMNS].sum(axis=1)==0]"
      ],
      "metadata": {
        "id": "nQsFQ7E3b80U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.concat([train_toxic,train_clean.sample(15000)])"
      ],
      "metadata": {
        "id": "nX86HX09b858"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=train_df.drop('id',axis=1)\n",
        "val_df=val_df.drop('id',axis=1)"
      ],
      "metadata": {
        "id": "1I6TYCX0b88U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.head(10)"
      ],
      "metadata": {
        "id": "6Xyluiptb8-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 0.00001"
      ],
      "metadata": {
        "id": "qO-rf_1Ab9BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "BPM_a4PCb9GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "GH9_HHO6b9JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index:int):\n",
        "        data_row=self.df.iloc[index]\n",
        "        comment_text=data_row.comment_text\n",
        "        targets=data_row[LABEL_COLUMNS]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(targets)\n",
        "        }"
      ],
      "metadata": {
        "id": "ECDqEAVCb9LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "Ug-B4ffCb9QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "9ZRKCLOkb9So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "4JkbrhiZb9V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, 6)\n",
        "    \n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids, \n",
        "            attention_mask=attn_mask, \n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "lrwnQ1Fvb9Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "AdIkyfkRb9b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_targets=[]\n",
        "val_outputs=[]"
      ],
      "metadata": {
        "id": "4xvpJK-ga0c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, optimizer):\n",
        "  for epoch in range(1,n_epochs+1):\n",
        "    train_loss=0\n",
        "    valid_loss=0\n",
        "    model.train()\n",
        "    print(\"Epoch {}\".format(epoch))\n",
        "    for batch_idx,data in enumerate(training_loader):\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        print(\"Trainng end epoch {}\".format(epoch))\n",
        "        print(\"Validation Start epoch {}\".format(epoch))\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          for batch_idx,data in enumerate(validation_loader,0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "          print(\"Validation end epoch {}\".format(epoch))\n",
        "          train_loss = train_loss/len(training_loader)\n",
        "          valid_loss = valid_loss/len(validation_loader)\n",
        "          print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "  return model"
      ],
      "metadata": {
        "id": "yCpsq-jfa0fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(EPOCHS,train_data_loader,val_data_loader,model,optimizer)"
      ],
      "metadata": {
        "id": "CgA_z1R9a0iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example=\"You are one of the best person I ever seen in my life that became devastating to my all expectations. Next time if I see your beautiful face I will frighen hell by your death.\"\n",
        "encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    add_special_tokens=True,\n",
        "    max_length=MAX_LEN,\n",
        "    padding='max_length',\n",
        "    return_token_type_ids=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "    attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "    output = model(input_ids, attention_mask, token_type_ids)\n",
        "    final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "    print(example)\n",
        "    print(\"Emotion\",\":\",train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])"
      ],
      "metadata": {
        "id": "twh9XKc-zvbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}